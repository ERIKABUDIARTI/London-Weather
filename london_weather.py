# -*- coding: utf-8 -*-
"""London_Weather.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rMiZCDVIQWn1RJ8lc4kw9wCjP4P6MXq0

Proyek TimeSeries: **London Weather Dataset**
- Nama:**ERIKA BUDIARTI**
- Email: erika.analytic@gmail.com
- Id Dicoding:erika_budiarti

# **Import Library**
"""

import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

import tensorflow as tf
from keras.optimizers import Adam
from keras.regularizers import l1_l2
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization
from keras.layers import LSTM, MaxPooling1D, Conv1D

"""# **Load Dataset**"""

df_london= pd.read_csv('/content/london_weather.csv')

df_london

df_london.shape

"""# **Data Cleaning**"""

selected_columns = df_london[['date','mean_temp']]
df_london = selected_columns.copy()

df_london.info()

df_london['date'] = pd.to_datetime(df_london['date'], format='%Y%m%d')
df_london['year'] = df_london['date'].dt.year
df_london['month'] = df_london['date'].dt.month
df_london['day'] = df_london['date'].dt.day

df_london.drop('date', axis=1, inplace=True)
df_london = df_london.rename(columns={'mean_temp': 'temperature'})

# Reorder columns as 'day', 'month', 'year', 'temperature'
df_london = df_london[['day', 'month', 'year', 'temperature']]

df_london

df_london.isna().sum()

mean_value = df_london['temperature'].mean()
df_london['temperature'].fillna(mean_value, inplace=True)

df_london.isna().sum()

df_london

"""# **Time Series Plot**"""

yearly_mean = df_london.groupby(['year'])['temperature'].mean().reset_index()

plt.figure(figsize=(20, 10))
plt.plot(yearly_mean['year'], yearly_mean['temperature'], marker='*', linestyle='-' , color='red')

min_year = yearly_mean['year'].min()
max_year = yearly_mean['year'].max()
plt.xlim((min_year-1), (max_year+1))
plt.xticks(yearly_mean['year'], rotation=45)

max_temperature = yearly_mean['temperature'].max()
min_temperature = yearly_mean['temperature'].min()
y_ticks = np.arange(min_temperature - 0.987, max_temperature + 0.323, 0.5)
plt.yticks(y_ticks)
plt.ylim((min_temperature -0.987), (max_temperature +0.323))

for i, row in yearly_mean.iterrows():
    plt.annotate(f'{row["temperature"]:.2f}', (row['year'], row['temperature']), textcoords="offset points", xytext=(0, 10), ha='center')

plt.title('Average Temperature Over Time')
plt.xlabel('Year')
plt.ylabel('Average Temperature')
plt.grid(True)
plt.tight_layout()
plt.show()

"""# **Split DataSet**"""

X = df_london[['month', 'year']]
y = df_london['temperature']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)

"""# **Build Model**"""

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    dataset = tf.data.Dataset.from_tensor_slices(series)
    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
    dataset = dataset.flat_map(lambda w: w.batch(window_size + 1))
    dataset = dataset.shuffle(shuffle_buffer)
    dataset = dataset.map(lambda w: (w[:-1], w[-1:]))
    return dataset.batch(batch_size).prefetch(1)

train_london = windowed_dataset(y_train, window_size=64, batch_size=100, shuffle_buffer=1000)
train_london = train_london.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.float32)))

val_london = windowed_dataset(y_test, window_size=64, batch_size=100, shuffle_buffer=1000)
val_london = val_london.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.float32)))

scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)

model = tf.keras.models.Sequential([
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True, input_shape=(64, 1))),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True, input_shape=(64, 1))),
    tf.keras.layers.LSTM(32, return_sequences=True),
    tf.keras.layers.LSTM(16, return_sequences=True),
    tf.keras.layers.Conv1D(32, 2, activation='relu'),
    tf.keras.layers.Conv1D(16, 2, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64, activation="relu", kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation="relu", kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(16, activation="relu", kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation="linear")
])

loss = 'mean_squared_error'

initial_learning_rate = 0.001
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps=1000, decay_rate=0.9)

optimizer = Adam(learning_rate=lr_schedule)

threshold_mae = (df_london['temperature'].max() - df_london['temperature'].min()) * 10/100
threshold_mae

class iCallback(tf.keras.callbacks.Callback):
    def __init__(self, threshold_mae):
        self.threshold_mae = threshold_mae

    def on_epoch_end(self, epoch, logs={}):
        if (logs.get('val_mae') < self.threshold_mae and
        logs.get('mae') < self.threshold_mae):
            print(f"\nReached threshold MAE ({self.threshold_mae}). Stopping training.")
            self.model.stop_training = True

callback_model = iCallback(threshold_mae)

model.compile(loss=loss,
              optimizer=optimizer,
              metrics=["mae"])

history = model.fit(train_london,
                    epochs = 10,
                    batch_size = 32,
                    validation_data = val_london,
                    callbacks=[callback_model],
                    verbose=2)

"""# **Plot Loss dan Accuracy**"""

# Data akurasi
train_accuracy = history.history['mae']
val_accuracy = history.history['val_mae']

# Data loss
train_loss = history.history['loss']
val_loss = history.history['val_loss']


fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Plot pertama (Akurasi)
axes[0].plot(train_accuracy, label='Train MAE')
axes[0].plot(val_accuracy, label='Validation MAE')
axes[0].set_title('Akurasi Model')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Accuracy')
axes[0].legend(loc='upper right')

# Plot kedua (Loss)
axes[1].plot(train_loss, label='Train Loss')
axes[1].plot(val_loss, label='Validation Loss')
axes[1].set_title('Loss Model')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].legend(loc='upper right')

plt.tight_layout()
plt.show()